{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/nikithapsrivatsa04/Infosys_Springboard/blob/main/Milestone2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o4wlS77Q_6xj"
   },
   "source": [
    "# 1. Setup and Installations\n",
    "Explanation: This initial section sets up the project environment by installing all the required libraries. It ensures that the user interface (ipywidgets) is functional, AI models can run efficiently (transformers), code quality and metrics can be analyzed (radon), and visualizations can be generated (seaborn). This step lays the foundation for seamless execution of subsequent modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lWDFV_v2wTfg",
    "outputId": "c2332d7d-02a3-4161-d81b-e27141cd1b2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing required libraries...\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.8/52.8 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\n",
      "Setup Complete! ✅\n"
     ]
    }
   ],
   "source": [
    "# --- Section 1: Setup & Installations ---\n",
    "print(\"Installing required libraries...\")\n",
    "!pip install transformers torch accelerate bitsandbytes pandas huggingface_hub radon ipywidgets matplotlib seaborn -q\n",
    "\n",
    "# Import necessary modules\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from huggingface_hub import notebook_login\n",
    "import ast\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from radon.complexity import cc_visit\n",
    "from radon.metrics import mi_visit\n",
    "from radon.raw import analyze\n",
    "\n",
    "print(\"\\nSetup Complete! ✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8QprWtII_-FS"
   },
   "source": [
    "# 2. Secure Hugging Face Login\n",
    "Explanation: This section manages authentication for accessing restricted or “gated” models on Hugging Face. Execute this cell and provide your Hugging Face access token when prompted to enable secure and authorized model usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EZfP9O6e__jn"
   },
   "source": [
    "# 3. Configuration & Backend Engine\n",
    "Explanation: This section sets up the project configuration by specifying the models to be tested and defining all backend helper functions. It includes the enhanced calculate_advanced_metrics function, which now computes detailed code quality metrics such as Halstead metrics, enabling comprehensive analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RfPJyfXkwbsG",
    "outputId": "21a0c061-7067-4493-a624-3ad952a3443b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backend engine with advanced metrics is ready.\n"
     ]
    }
   ],
   "source": [
    "# --- Section 3: Configuration & Backend Engine ---\n",
    "# --- Model Configuration ---\n",
    "MODELS_TO_TEST = {\n",
    "    \"DeepSeek-Coder-1.3B\": \"deepseek-ai/deepseek-coder-1.3b-instruct\",\n",
    "    \"Phi-2-2.7B\": \"microsoft/phi-2\",\n",
    "    \"Gemma-2B-IT\": \"google/gemma-2b-it\",\n",
    "}\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# --- Helper & Generation Functions ---\n",
    "def clean_generated_code(text, model_path):\n",
    "    model_path = model_path.lower()\n",
    "    if \"gemma\" in model_path: text = re.sub(r\"<start_of_turn>user\\n.*<end_of_turn>\\n<start_of_turn>model\\n\", \"\", text, flags=re.DOTALL).replace(\"<end_of_turn>\", \"\")\n",
    "    elif \"phi-2\" in model_path: text = re.sub(r\"Instruct:.*\\nOutput:\", \"\", text, flags=re.DOTALL)\n",
    "    else: text = re.sub(r\"### Instruction:\\n.*\\n\\n### Response:\", \"\", text, flags=re.DOTALL)\n",
    "    match = re.search(r\"```python\\n(.*?)\\n```\", text, re.DOTALL)\n",
    "    if match: text = match.group(1)\n",
    "    return text.strip()\n",
    "\n",
    "def is_syntactically_valid(code_string: str) -> bool:\n",
    "    if not code_string: return False\n",
    "    try: ast.parse(code_string); return True\n",
    "    except SyntaxError: return False\n",
    "\n",
    "def calculate_advanced_metrics(code_string):\n",
    "    if not is_syntactically_valid(code_string):\n",
    "        return {\"complexity\": None, \"maintainability\": None, \"loc\": None}\n",
    "    try:\n",
    "        complexity = sum([c.complexity for c in cc_visit(code_string)]) if cc_visit(code_string) else 0\n",
    "        maintainability = mi_visit(code_string, multi=True)\n",
    "        loc = analyze(code_string).loc\n",
    "        return {\"complexity\": complexity, \"maintainability\": round(float(maintainability), 2), \"loc\": loc}\n",
    "    except: return {\"complexity\": None, \"maintainability\": None, \"loc\": None}\n",
    "\n",
    "def generate_code(model, tokenizer, prompt):\n",
    "    model_path = model.name_or_path.lower()\n",
    "    if \"gemma\" in model_path: formatted_prompt = f\"<start_of_turn>user\\n{prompt}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
    "    elif \"phi-2\" in model_path: formatted_prompt = f\"Instruct: {prompt}\\nOutput:\"\n",
    "    else: formatted_prompt = f\"### Instruction:\\n{prompt}\\n\\n### Response:\"\n",
    "    if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\", return_attention_mask=True).to(device)\n",
    "    start_time = time.time()\n",
    "    output_ids = model.generate(inputs.input_ids, attention_mask=inputs.attention_mask, max_new_tokens=512, temperature=0.1, do_sample=True, pad_token_id=tokenizer.pad_token_id)\n",
    "    end_time = time.time()\n",
    "\n",
    "    raw_output = tokenizer.batch_decode(output_ids)[0]\n",
    "    cleaned_code = clean_generated_code(raw_output, model_path)\n",
    "\n",
    "    return {\"code\": cleaned_code, \"gen_time\": end_time - start_time}\n",
    "\n",
    "print(\"Backend engine with advanced metrics is ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yiTGlRaBABYr"
   },
   "source": [
    "# 4. Pre-Loading All AI Models\n",
    "Explanation: This section loads all AI models into memory in advance. While this process may take several minutes, it ensures a significantly faster and smoother user interface experience. Note: Pre-loading all models will consume a substantial portion of your GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 341,
     "referenced_widgets": [
      "d1ee0858904b480dabf205baf80fc800",
      "7f88947adad746c7a131cb1166945da8",
      "8e8a7e9e3d384f238882f53550e1e54b",
      "c4a9f3dca80e4effbd137eb450a8b2d9",
      "89f366111f2d41f3816f606561e87234",
      "a061e7f3e27f42e9ac312f241496fc26",
      "7ab044ae428c47d9b7ffc9615147323e",
      "620500309310448fbb6c5bff8034e712",
      "8c90667933c442dbbb928f4caa4cbba9",
      "e1b54e16a6ef40178801bd4285f0c2b1",
      "f1a09ab4c3794baba09a29b3c9bb7980",
      "ae879be5e9124fc7bfb93b4eb5c30190",
      "e80b868e0a5747bd941682cfdf7d042b",
      "8180cd8c79494fa8b6603a7d46a4657f",
      "10fac8a7254c4389968051c3f6a27fdd",
      "514174a09d1540a1a9165b65795568f0",
      "eb9c825532bd4e79bc328fe800942f54",
      "847afbb8d85e49ee8a5d3ebbe1bcf7a7",
      "ca17980deb5b4f0c9c657148b393c781",
      "c41dfc0ed0c5478d9e9c906fd5116b0a",
      "36fad697b2074c15a1864df7ac73e739",
      "19544564c8094ce6ba78bb59f4407515"
     ]
    },
    "id": "lKKZieVbzubw",
    "outputId": "90207020-8132-47d4-a002-9a1b69ebe5b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to pre-load all models...\n",
      "\n",
      "--- Loading DeepSeek-Coder-1.3B... ---\n",
      "✅ DeepSeek-Coder-1.3B loaded successfully.\n",
      "\n",
      "--- Loading Phi-2-2.7B... ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1ee0858904b480dabf205baf80fc800",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Phi-2-2.7B loaded successfully.\n",
      "\n",
      "--- Loading Gemma-2B-IT... ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae879be5e9124fc7bfb93b4eb5c30190",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Gemma-2B-IT loaded successfully.\n",
      "\n",
      "==================================================\n",
      "All available models are pre-loaded.\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# --- Section 4: Pre-Loading All Models ---\n",
    "loaded_models = {}\n",
    "print(\"Starting to pre-load all models...\")\n",
    "for model_name, model_path in MODELS_TO_TEST.items():\n",
    "    print(f\"\\n--- Loading {model_name}... ---\")\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.bfloat16, device_map=\"auto\", trust_remote_code=True)\n",
    "        loaded_models[model_name] = {\"model\": model, \"tokenizer\": tokenizer}\n",
    "        print(f\"✅ {model_name} loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ FAILED to load {model_name}. Error: {e}\")\n",
    "print(\"\\n\" + \"=\"*50 + \"\\nAll available models are pre-loaded.\\n\" + \"=\"*50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VM2x5sRaADvj"
   },
   "source": [
    "# 5. UI #1: Benchmark All Models\n",
    "Explanation: This interface enables broad comparative benchmarking across all pre-loaded models. By entering a single prompt, the system evaluates each model and presents the results along with relevant performance metrics in a clear, tabular format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3218,
     "referenced_widgets": [
      "4d9b9b2a579c4adfb7472bb879fcba0a",
      "f1bce25cebc8424bac5fd804ea11d32e",
      "135f34fd5f2a4d8a95794ad035831bea",
      "7b877cc42f314f498713dac787551780",
      "ae5cfd4b906d452e82d6c03d5ca92bc0",
      "f327faec79d142f6ad3ebeb6468f1859",
      "c009781b3ed4445fa4ca291a56f4e972",
      "e1ab85271b9c41b3bdf616175c2955fe",
      "feca7c862d754d80b711aa22b3680aa9",
      "895a8f92c7e84feb88a0f7f149512582"
     ]
    },
    "id": "IaRnRpaHzxtT",
    "outputId": "ff108676-8203-4e27-8d8c-019682acae33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- UI #1: Benchmark All Models ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d9b9b2a579c4adfb7472bb879fcba0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Textarea(value='', layout=Layout(width='95%'), placeholder='Enter a prompt to benchmark all mod…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Section 5: UI #1 - Run All Models ---\n",
    "print(\"--- UI #1: Benchmark All Models ---\")\n",
    "all_results_log = [] # Global log to store all results from both UIs\n",
    "\n",
    "prompt_input_all = widgets.Textarea(placeholder='Enter a prompt to benchmark all models...', layout={'width': '95%'})\n",
    "run_all_button = widgets.Button(description='Run Benchmark', button_style='danger', icon='rocket')\n",
    "output_all = widgets.Output(layout={'border': '1px solid black', 'padding': '10px', 'overflow': 'scroll'})\n",
    "\n",
    "def on_run_all_clicked(b):\n",
    "    with output_all:\n",
    "        prompt = prompt_input_all.value\n",
    "        if not prompt: print(\"Please enter a prompt.\"); return\n",
    "\n",
    "        print(f\"Running prompt on {len(loaded_models)} models...\")\n",
    "        results_this_run = []\n",
    "        for model_name, components in loaded_models.items():\n",
    "            print(f\"  - Generating with {model_name}...\")\n",
    "            result = generate_code(components['model'], components['tokenizer'], prompt)\n",
    "            metrics = calculate_advanced_metrics(result['code'])\n",
    "\n",
    "            entry = {'Model': model_name, 'Prompt': prompt, **result, **metrics}\n",
    "            results_this_run.append(entry)\n",
    "            all_results_log.append(entry)\n",
    "\n",
    "        print(\"\\n--- Benchmark Complete ---\")\n",
    "        results_df = pd.DataFrame(results_this_run).round(2)\n",
    "        display(HTML(results_df.to_html().replace('\\\\n', '<br>')))\n",
    "\n",
    "run_all_button.on_click(on_run_all_clicked)\n",
    "display(widgets.VBox([prompt_input_all, run_all_button, output_all]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_lqQ6GVuHTaB"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VujxCvd3AF7t"
   },
   "source": [
    "# 6. UI #2: Inspect Models with Checkboxes\n",
    "Explanation: This interface offers greater control over model selection. Each pre-loaded model is represented by a checkbox, allowing you to run a prompt on a specific subset of models, rather than all models at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 514,
     "referenced_widgets": [
      "941bbd52462f4711b90037e9f4af536e",
      "4c05b1021da44513badd8f687f1cbf0d",
      "08399296e8e443599b6ebff1413861c0",
      "4575cb23f77949aeb99abca489a79841",
      "d705daea8aa44502ab5b6824761b2cad",
      "cb7e86e93f5742c595a6da70a1a11a0a",
      "e9a1ebfc2d1d421c94108851640780c9",
      "e5c6d12a3aa949b69b2754c4dcff310a",
      "58811131f68f4ff9ad87fbe2a8bb76d7",
      "b29fefe6f1a1488cbe60d2eb30b488c4",
      "e2233cbf40f4484b8eb27be371f10efb",
      "e487aa5db9314f7e8d16caf23576bee1",
      "7219520f0e4e4dad9f7563ccf0eb892a",
      "4d3e681812ae48fab1f9902fe013360c",
      "9cbfbb6779a74233849c362dc60b5fc0",
      "f428b72fd0bc44a08c0f950d3f13dc0b",
      "7470d70b3d6648a89fc02f6cb7deebde",
      "a3150be61da74b629eb416c55a6bc022",
      "b92ecaadc0074e04a7bbeb795552c82c",
      "e1de43271a4145b0bfd969d9059c28f5",
      "990aa39f1407495e8389a527dc5beaa6",
      "65990af53bde480b912bbe66adc0eaf7",
      "d95519f675164f4896172bb53a36c82d",
      "9a480e090b3e4edda3289fe206014918"
     ]
    },
    "id": "ZAEZP9V33INQ",
    "outputId": "0ea99618-d08f-4bf4-c2ec-c7789feebc1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- UI #2: Inspect Selected Models ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "941bbd52462f4711b90037e9f4af536e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Textarea(value='', layout=Layout(width='95%'), placeholder='Enter a prompt for selected models.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Section 6: UI #2 - Run on Selected Models ---\n",
    "print(\"\\n\\n--- UI #2: Inspect Selected Models ---\")\n",
    "\n",
    "prompt_input_selected = widgets.Textarea(placeholder='Enter a prompt for selected models...', layout={'width': '95%'})\n",
    "run_selected_button = widgets.Button(description='Run Selected', button_style='success', icon='play')\n",
    "output_selected = widgets.Output(layout={'border': '1px solid black', 'padding': '10px', 'overflow': 'scroll'})\n",
    "\n",
    "model_checkboxes = {name: widgets.Checkbox(value=True, description=name) for name in loaded_models.keys()}\n",
    "checkbox_container = widgets.VBox(list(model_checkboxes.values()))\n",
    "\n",
    "def on_run_selected_clicked(b):\n",
    "    with output_selected:\n",
    "        output_selected.clear_output(wait=True)\n",
    "        prompt = prompt_input_selected.value\n",
    "        if not prompt: print(\"Please enter a prompt.\"); return\n",
    "\n",
    "        models_to_run = [name for name, cb in model_checkboxes.items() if cb.value]\n",
    "        if not models_to_run: print(\"Please select at least one model.\"); return\n",
    "\n",
    "        print(f\"Running prompt on {len(models_to_run)} selected models...\")\n",
    "        results_this_run = []\n",
    "        for model_name in models_to_run:\n",
    "            print(f\"  - Generating with {model_name}...\")\n",
    "            components = loaded_models[model_name]\n",
    "            result = generate_code(components['model'], components['tokenizer'], prompt)\n",
    "            metrics = calculate_advanced_metrics(result['code'])\n",
    "\n",
    "            entry = {'Model': model_name, 'Prompt': prompt, **result, **metrics}\n",
    "            results_this_run.append(entry)\n",
    "            all_results_log.append(entry)\n",
    "\n",
    "        print(\"\\n--- Selected Run Complete ---\")\n",
    "        results_df = pd.DataFrame(results_this_run).round(2)\n",
    "        display(HTML(results_df.to_html().replace('\\\\n', '<br>')))\n",
    "\n",
    "run_selected_button.on_click(on_run_selected_clicked)\n",
    "ui_selected_models = widgets.VBox([prompt_input_selected, widgets.HTML(\"<h4>Select models to run:</h4>\"), checkbox_container, run_selected_button, output_selected])\n",
    "display(ui_selected_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZbvFJAdRAJNH"
   },
   "source": [
    "# 7. Final Analysis and Visualization Report\n",
    "Explanation: After generating results using either UI, this section produces a comprehensive report. Clicking the button generates a complete data table along with comparative visualizations of key metrics, providing a clear overview of all tests executed during your session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1255,
     "referenced_widgets": [
      "3a05aae093c14e1b921001e82a3e79b4",
      "5ccebdbb6a9d4bb3be09833195d6752b",
      "faebcf7735bd4aea9cf396239091c83b",
      "3a780ca6a215445a86e2b91959260180",
      "636bb3834eb4473e9ba42be6b9555300",
      "84ddaa6b90e04422b7c75106eba20d21",
      "8f3b371701c24b7eb92dce9fe5b9198d"
     ]
    },
    "id": "KWTvTm_x3dmI",
    "outputId": "addc87ce-959e-4782-bd46-e62f7a3c8f08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Use the button below to generate the final report for the session.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a05aae093c14e1b921001e82a3e79b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Button(button_style='info', description='Generate Full Report & Plots', style=ButtonStyle()), O…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Section 7: Final Analysis and Visualization Report ---\n",
    "report_button = widgets.Button(description=\"Generate Full Report & Plots\", button_style='info')\n",
    "report_output = widgets.Output()\n",
    "\n",
    "def on_report_button_clicked(b):\n",
    "    with report_output:\n",
    "        report_output.clear_output(wait=True)\n",
    "        if not all_results_log:\n",
    "            print(\"No results logged. Use one of the UIs above to generate code.\")\n",
    "            return\n",
    "\n",
    "        df = pd.DataFrame(all_results_log).round(2)\n",
    "        df.rename(columns={'complexity': 'Complexity', 'maintainability': 'Maintainability', 'gen_time': 'Gen Time (s)'}, inplace=True)\n",
    "\n",
    "        print(\"--- Full Session Data ---\")\n",
    "        display(df)\n",
    "\n",
    "        print(\"\\n--- Comparative Plots ---\")\n",
    "        sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "        plot_df = df.dropna(subset=['Complexity', 'Maintainability'])\n",
    "\n",
    "        if plot_df.empty:\n",
    "            print(\"Not enough valid data to generate plots.\")\n",
    "            return\n",
    "\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "        fig.suptitle('Comparative Analysis of Code Metrics', fontsize=16)\n",
    "\n",
    "        sns.barplot(ax=axes[0], data=plot_df, x='Model', y='Gen Time (s)', palette='viridis')\n",
    "        axes[0].set_title('Generation Time (Lower is Faster)')\n",
    "        axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "        sns.barplot(ax=axes[1], data=plot_df, x='Model', y='Complexity', palette='magma')\n",
    "        axes[1].set_title('Cyclomatic Complexity (Lower is Simpler)')\n",
    "        axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "        sns.barplot(ax=axes[2], data=plot_df, x='Model', y='Maintainability', palette='plasma')\n",
    "        axes[2].set_title('Maintainability Index (Higher is Better)')\n",
    "        axes[2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        plt.show()\n",
    "\n",
    "report_button.on_click(on_report_button_clicked)\n",
    "print(\"\\nUse the button below to generate the final report for the session.\")\n",
    "display(widgets.VBox([report_button, report_output]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hj8Pye0GANRG"
   },
   "source": [
    "# 8. (Optional) Manual Cleanup\n",
    "Explanation: This optional step allows you to manually clear all pre-loaded models from memory, freeing up GPU resources and ensuring efficient system performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u1dA5qBz3hpT"
   },
   "outputs": [],
   "source": [
    "# --- Section 8: Optional Manual Cleanup ---\n",
    "def clear_all_models():\n",
    "    global loaded_models\n",
    "    print(f\"Clearing {len(loaded_models)} models from memory...\")\n",
    "    for model_name in list(loaded_models.keys()):\n",
    "        del loaded_models[model_name]['model']\n",
    "        del loaded_models[model_name]['tokenizer']\n",
    "        del loaded_models[model_name]\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"\\n✅ All models have been cleared from GPU memory.\")\n",
    "\n",
    "# To run the cleanup, uncomment and run the line below:\n",
    "clear_all_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cDtsZdGG4mWv"
   },
   "source": [
    "# Algorithms & Logic\n",
    "\n",
    "\n",
    "1. Write a Python function is_palindrome(s) that returns True if a string is a\n",
    "palindrome, ignoring spaces, punctuation, and case.\n",
    "\n",
    "2. Write a Python function find_common_elements(list1, list2) that returns a sorted list of elements present in both input lists without duplicates.\n",
    "\n",
    "3. Implement a Python Stack class with methods push, pop, peek, is_empty, and a method to return the stack size.\n",
    "\n",
    "# Data Manipulation\n",
    "\n",
    "1. Write a Python function get_unique_even_numbers(numbers) that takes a list of integers and returns a sorted list of unique even numbers using a list comprehension.\n",
    "\n",
    "2. Write a Python function merge_dictionaries(d1, d2) that merges two dictionaries. If a key exists in both, the value from the second dictionary overwrites the first.\n",
    "\n",
    "3. Write a Python function filter_positive_numbers(numbers) that returns a new list containing only the positive numbers from the input list.\n",
    "\n",
    "# File Handling & Web APIs\n",
    "\n",
    "1. Write a Python function count_words_in_file(filepath) that reads a text file and returns the total number of words, ignoring punctuation.\n",
    "\n",
    "2. Write a Python function get_weather(city) that uses the requests library to fetch the current weather for a given city from the OpenWeatherMap API (https://api.openweathermap.org/data/2.5/weather) and returns the temperature in Celsius.\n",
    "\n",
    "3. Write a Python function read_csv_and_sum_column(filepath, column_name) that reads a CSV file and returns the sum of values in the specified column.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fB925uuFARss"
   },
   "source": [
    "# Overall Objective:\n",
    "To gain proficiency in ipywidgets by designing, styling, and implementing a comprehensive, multi-tab application. The project aims to include an interactive chat interface, file-handling utilities, and other feature-rich components, demonstrating practical mastery of dynamic UI development in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NW2udrhRC2u9"
   },
   "source": [
    "# 1. Introduction & Setup\n",
    "This initial section prepares the project environment. It imports ipywidgets to create interactive user interfaces, IPython.display for rendering HTML and rich content, and essential libraries such as numpy and matplotlib to support the data plotting tools that will be developed later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iUtApvbD4kOl",
    "outputId": "f063a8a6-07d4-4527-9b93-94df4e6b3301"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete. All necessary libraries are ready.\n"
     ]
    }
   ],
   "source": [
    "# --- Section 1: Setup & Imports ---\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "import io\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Setup complete. All necessary libraries are ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WWhzRkPyAT10"
   },
   "source": [
    "# 2. Core Widget Showcase\n",
    "Before developing the full application, it is essential to understand the fundamental building blocks. This section demonstrates a variety of common ipywidgets, highlighting their appearance and the types of input they accept. Many of these widgets will later be integrated into the application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226,
     "referenced_widgets": [
      "e444b77f795e4245905584fc996a8900",
      "85671d187bf24c8682ab01b9650414ec",
      "d98e72b62caa4e7abce57e179cd1fe48",
      "8c6d0f6a3a3b4082a8560bb9687b8c21",
      "a7a933e196e64962b95f2f507a818954",
      "f014047fcea146c893d6bf2842ab9bd7",
      "eebc1721483747d98cdbadc109786076",
      "beea78c232b6491684a5ef6c27be26ad",
      "77d66bfeb46e4cdaa20c9c1cef269c5e",
      "deda72136d794e45abf6faeaa12fc3ce",
      "ae407afb1494459ca0a527ec0becff9d",
      "fa7ce653d1d1440382830dfd828e7363",
      "0594425249e441a8b28e31371852fe5a",
      "01d4d02473d74930ba5742f8a9a97fc8",
      "fcc0f11c97fe419498c66dd0b651019c",
      "462636d4baf047bb99fa87ee391395ee",
      "0e4a8a0315ba48f1a88b3737f4e7cc4a",
      "8430dfbfa45a42dfab0b46cdbee78435",
      "e2285ffa7be549fb957b98252c7aba9c",
      "28d2f4e696ec42aab3f25cfb910833bc"
     ]
    },
    "id": "20ZGShTi4s2p",
    "outputId": "2ab720e0-92ac-483b-9c0a-945c90a68bcb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- A Showcase of Common Widgets ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e444b77f795e4245905584fc996a8900",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Text(value='', description='Name:'), IntSlider(value=7, description='Count:', max=10), Checkbox…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Section 2: Core Widget Showcase ---\n",
    "print(\"--- A Showcase of Common Widgets ---\")\n",
    "\n",
    "# Create a collection of different widgets to demonstrate their functionality\n",
    "text_widget = widgets.Text(description='Name:')\n",
    "slider_widget = widgets.IntSlider(value=7, min=0, max=10, description='Count:')\n",
    "checkbox_widget = widgets.Checkbox(value=True, description='Confirm?')\n",
    "dropdown_widget = widgets.Dropdown(options=['Analysis', 'Training', 'Deployment'], description='Phase:')\n",
    "date_widget = widgets.DatePicker(description='Select Date')\n",
    "color_widget = widgets.ColorPicker(description='Pick Color', value='skyblue')\n",
    "\n",
    "# Display them all in a vertical box for a clean presentation\n",
    "showcase_box = widgets.VBox([\n",
    "    text_widget, slider_widget, checkbox_widget,\n",
    "    dropdown_widget, date_widget, color_widget\n",
    "])\n",
    "display(showcase_box)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w6FSA0_tAW4m"
   },
   "source": [
    "# 3. Designing a Styled Chat Interface\n",
    "This section focuses on building the visual elements of the main chat interface. The Layout widget is used to manage size, borders, and margins, similar to CSS styling. A styled header is created with the HTML widget, and a “Clear” button is added to enhance user experience. All components are then assembled into a single layout variable, chatbot_tab_content, forming the complete chat interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 471,
     "referenced_widgets": [
      "bacbf8a4d1a94cd3be76cc7abf150ca3",
      "76c8944a392e49c785bcf9701bf06af2",
      "07b1d661953144c990cc4124808ff84e",
      "8d1ee713400042d6a909bb3647d5246b",
      "02a02386800a46cc84be737073af712c",
      "453fbde356a440a88105d9de3b70645f",
      "9a87dc077a5f408b9f9160700cfd80ed",
      "2f23fcae61084eaf824a985f87c52d4b",
      "5a596642cadc4f2daa80eefb99f6ec45",
      "a4abee2dc3c14bcf90a7e7b9079032e2",
      "9fa960becc804c8d94f2df1704a441f6",
      "daee5170fa85456f8f872a832c25c202",
      "659277ce50814f88bc533562662cc7ef",
      "ab08eb512759400bac858c2a8371ae35",
      "59425d31cd5c4d9eb3f7ed08136858d0",
      "8ca0ded7235b46bc9dd722edd7849dcc",
      "4d75623044984b77bb80f72ac5f0473a",
      "e35f0d6d38a442bf94244e8e51bd9c6f",
      "7c335c9f078448faaf361e3a4edc9a37",
      "2d8635adda9f43378efa705fd1a68b9d",
      "cf48a85484ec4d9ea725937b2c840a24",
      "ea4ae5c47dee45c193fc03c21f7c15fa",
      "d3c5469582bb4473b9a63a56c4187fb2",
      "e6b9ae7149fb4dc3a2cbeb4be504d249",
      "0769d551ba3d490a826c1a47104e254f",
      "f5939748d2b04f6cadd85ddc997b9a33",
      "7c075f037e994aafbc0acc11028dc880",
      "38c48edba94e43b38c01ad32ffb7e447",
      "2befeacbddfa4fa282eca9dbb52aca2e",
      "0a046b73b901473682cf19eb3c232060",
      "ad94abd679804fe684b0a54eaecca8b6",
      "0d939f94ee7440af85b7bc036057f0b5",
      "bc411273954d4a9ba913aedd10bd486b",
      "77373ea666c94d34a6113ff428062341",
      "10f6ee7d8a2f48d4b34aa1d51e4df615",
      "247a3fd5c8684c6096a33c02c2ad6796",
      "167ac1892ea54a99a779693780643de0",
      "f79ce3e85ddb42febb5ce5dc7c0460cc",
      "46e1f2e9dce640e5a8f3be154677f5aa",
      "abeba0d44f884bf6840c5fc611166dea",
      "6a38cad4e4a14305a56e41ef591e13cd",
      "6c8343d6b5dc480ebc00c07b56bc6c9c",
      "cfacc3f3f44e48e49e18536a3f02fdbb",
      "496e3d779ba949e7b25ca817c67a8165",
      "4799c4b2e57643d7a8dcfd84bbced34d",
      "a9d87c6902104f899145684b163546aa",
      "ce47fc78376345f7b1139e654b3b43e8",
      "89107aed943b4533b2e1344192caff87",
      "921eaafd039f4ab5a2df65c7fead04cd",
      "64c16b0d31ce47a4a8287412ba5a8421",
      "178d661b7495430d91f142e186a5064b",
      "daad0608f2e54831be2cb89bc8d30d80",
      "cb43fccec627406e8af5357199a51471",
      "ad17d1fe1eab4f9084879171d0726816"
     ]
    },
    "id": "JAV1GQrH8YwB",
    "outputId": "06d13b26-39f2-465e-e8b5-5308e6fee47b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Building Styled Chat UI Components ---\n",
      "Styled chat UI is assembled and ready to be linked to logic.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bacbf8a4d1a94cd3be76cc7abf150ca3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=\"<h1 style='color: #4A90E2;'>Interactive Chat App</h1>\"), Output(layout=Layout(borde…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import time\n",
    "\n",
    "# --- UI Setup ---\n",
    "print(\"--- Building Styled Chat UI Components ---\")\n",
    "\n",
    "header_html = widgets.HTML(value=\"<h1 style='color: #4A90E2;'>Interactive Chat App</h1>\")\n",
    "\n",
    "chat_history = widgets.Output(\n",
    "    layout=widgets.Layout(\n",
    "        height='300px',\n",
    "        border='1px solid #ccc',\n",
    "        padding='10px',\n",
    "        overflow='scroll'\n",
    "    )\n",
    ")\n",
    "\n",
    "prompt_input_styled = widgets.Textarea(\n",
    "    placeholder='Type your message...',\n",
    "    layout=widgets.Layout(width='calc(100% - 180px)')\n",
    ")\n",
    "send_button_styled = widgets.Button(\n",
    "    description='Send',\n",
    "    button_style='info',\n",
    "    icon='paper-plane',\n",
    "    layout=widgets.Layout(width='100px')\n",
    ")\n",
    "clear_button = widgets.Button(\n",
    "    description='Clear',\n",
    "    button_style='warning',\n",
    "    icon='trash',\n",
    "    layout=widgets.Layout(width='70px')\n",
    ")\n",
    "\n",
    "input_bar_styled = widgets.HBox([prompt_input_styled, send_button_styled, clear_button])\n",
    "chatbot_tab_content = widgets.VBox([header_html, chat_history, input_bar_styled])\n",
    "print(\"Styled chat UI is assembled and ready to be linked to logic.\")\n",
    "display(chatbot_tab_content)\n",
    "\n",
    "# --- Bot Logic ---\n",
    "def get_bot_reply(user_message):\n",
    "    msg = user_message.lower()\n",
    "\n",
    "    # --- Greetings ---\n",
    "    if any(greet in msg for greet in [\"hello\", \"hi\", \"hey\"]):\n",
    "        return \"Hello there! How can I help you today?\"\n",
    "\n",
    "    # --- How are you ---\n",
    "    elif any(phrase in msg for phrase in [\"how are you\", \"how's it going\"]):\n",
    "        return \"I'm just a bot, but I'm doing great! How about you?\"\n",
    "\n",
    "    # --- Farewell ---\n",
    "    elif any(bye in msg for bye in [\"bye\", \"goodbye\", \"see you\"]):\n",
    "        return \"Goodbye! Have a nice day!\"\n",
    "\n",
    "    # --- Coding questions ---\n",
    "    elif \"python\" in msg:\n",
    "        return (\"Python is a popular programming language. \"\n",
    "                \"You can define a function like this:\\n\"\n",
    "                \"`def my_function():\\n    print('Hello')`\")\n",
    "    elif \"loop\" in msg or \"for loop\" in msg or \"while loop\" in msg:\n",
    "        return (\"Loops repeat actions. Example in Python:\\n\"\n",
    "                \"`for i in range(5):\\n    print(i)`\")\n",
    "    elif \"list\" in msg:\n",
    "        return (\"A list in Python stores multiple items:\\n\"\n",
    "                \"`my_list = [1, 2, 3]`\")\n",
    "    elif \"ai\" in msg or \"machine learning\" in msg:\n",
    "        return (\"AI is the simulation of human intelligence by machines. \"\n",
    "                \"Machine learning allows systems to learn from data automatically.\")\n",
    "\n",
    "    # --- Default response for unknown queries ---\n",
    "    else:\n",
    "        return \"Interesting! Can you tell me more or ask me something about coding or AI?\"\n",
    "\n",
    "def send_message(b):\n",
    "    message = prompt_input_styled.value.strip()\n",
    "    if not message:\n",
    "        return\n",
    "    prompt_input_styled.value = \"\"\n",
    "\n",
    "    # Display user message\n",
    "    with chat_history:\n",
    "        display(widgets.HTML(f\"<b>You:</b> {message}\"))\n",
    "\n",
    "    # Display bot typing simulation\n",
    "    with chat_history:\n",
    "        typing_display = widgets.HTML(\"<i>Bot is typing...</i>\")\n",
    "        display(typing_display)\n",
    "\n",
    "    time.sleep(1)  # simulate typing delay\n",
    "\n",
    "    # Show bot reply\n",
    "    with chat_history:\n",
    "        typing_display.value = \"\"\n",
    "        reply = get_bot_reply(message)\n",
    "        display(widgets.HTML(f\"<b>Bot:</b> {reply}\"))\n",
    "\n",
    "def clear_chat(b):\n",
    "    chat_history.clear_output()\n",
    "\n",
    "send_button_styled.on_click(send_message)\n",
    "clear_button.on_click(clear_chat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ufa4Ei6AZ0f"
   },
   "source": [
    "# 4. Implementing the Interactive Chat Logic\n",
    "This section develops the core functionality of the chatbot. A mock backend function, get_bot_response, is used to simulate replies. Two handler functions are created: on_send_button_clicked processes user input and updates the conversation, while on_clear_button_clicked clears the chat history. The send handler also includes a “Bot is typing…” indicator to enhance the responsiveness and realism of the UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_01z-jhP42AC",
    "outputId": "06c1889d-8438-418f-cd96-68b6885fad5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ The chatbot UI is now fully interactive!\n"
     ]
    }
   ],
   "source": [
    "# --- Section 4: Chatbot Logic and Handlers ---\n",
    "def get_bot_response(user_text):\n",
    "    \"\"\"A mock backend function with more keyword-based replies.\"\"\"\n",
    "    user_text_lower = user_text.lower()\n",
    "    if \"hello\" in user_text_lower: return \"Hello there! How can I assist you today?\"\n",
    "    if \"style\" in user_text_lower: return \"Styling uses `widgets.Layout` for size/borders and `HTML` for rich text.\"\n",
    "    if \"file\" in user_text_lower: return \"You can handle files with the 'File Inspector' and 'Multi-File Viewer' tabs.\"\n",
    "    if \"widget\" in user_text_lower: return \"This UI is built with `ipywidgets`! We are using VBox, HBox, Tab, Button, and Output widgets.\"\n",
    "    if \"help\" in user_text_lower: return \"You can ask me about 'hello', 'style', 'file', or 'widget'.\"\n",
    "    return f\"I am a simple mock bot. You said: '{user_text}'.\"\n",
    "\n",
    "# Handler for the \"Send\" button\n",
    "def on_send_button_clicked(b):\n",
    "    user_text = prompt_input_styled.value\n",
    "    if not user_text: return\n",
    "\n",
    "    with chat_history:\n",
    "        display(HTML(f\"<div style='background-color: #E1F5FE; padding: 8px; border-radius: 5px; margin: 5px;'><b>You:</b> {user_text}</div>\"))\n",
    "\n",
    "    send_button_styled.disabled = True\n",
    "    clear_button.disabled = True\n",
    "    with chat_history:\n",
    "        display(HTML(f\"<div style='background-color: #F1F8E9; padding: 8px; border-radius: 5px; margin: 5px;'><i>Bot is typing...</i></div>\"))\n",
    "    time.sleep(1) # Simulate a processing delay\n",
    "\n",
    "    bot_text = get_bot_response(user_text)\n",
    "    with chat_history:\n",
    "        chat_history.outputs = chat_history.outputs[:-1] # Remove the \"typing...\" message\n",
    "        display(HTML(f\"<div style='background-color: #F1F8E9; padding: 8px; border-radius: 5px; margin: 5px;'><b>Bot:</b> {bot_text}</div>\"))\n",
    "\n",
    "    prompt_input_styled.value = \"\"\n",
    "    send_button_styled.disabled = False\n",
    "    clear_button.disabled = False\n",
    "\n",
    "# Handler for the \"Clear\" button\n",
    "def on_clear_button_clicked(b):\n",
    "    chat_history.clear_output()\n",
    "\n",
    "# Link the handlers to the button click events\n",
    "send_button_styled.on_click(on_send_button_clicked)\n",
    "clear_button.on_click(on_clear_button_clicked)\n",
    "\n",
    "print(\"✅ The chatbot UI is now fully interactive!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g0E_6Nh9AcJk"
   },
   "source": [
    "# 5. Building the \"File Inspector\" Tool\n",
    "This section introduces the second tool of the application. It uses a FileUpload widget for single-file uploads, accompanied by a button and an output area. The handler function is designed to be generic: it reads the file’s metadata (name, size, type) and attempts to display a text preview when possible. This implementation demonstrates how to handle and inspect various types of files effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 140,
     "referenced_widgets": [
      "b6da150d87b34f629ec699c9a26c7c66",
      "ef8c435f9f074d61814005ef17bc2220",
      "6af0ca67da3e4b608e7d1f2dd7de547c",
      "91f94305dcda47acb8637d3e20f69ae1",
      "8fc677c33fe347b7a74d14cae1413788",
      "f74166e407e34df5b0e2269135c3a4b2",
      "87f2cff7d1814555a31ebb3ed15cd408",
      "3bcb508075314033b0dcfb556677e9fa",
      "9bc16da709874d098fed49239da1bbe1",
      "0cc4e73b67024588ac54066dbe65ca2b",
      "cc96874808f347278bca8c410d9f0133",
      "c4ffeb8d832643408fbe81a2220794d0",
      "31388715cae34a99bb32f2c2a8d8b166",
      "f1e90031583a440280e4739905232ad4",
      "d4822f78569e4391a0a34a602e64b6b2"
     ]
    },
    "id": "IA0RJ_qw47Ry",
    "outputId": "9077d1ed-be30-4363-bcad-f1d427ea0e2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Inspector tool created and is interactive.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6da150d87b34f629ec699c9a26c7c66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h3>Generic File Inspector</h3>'), HBox(children=(FileUpload(value={}, accept='*/*'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Section 5: File Inspector Tool ---\n",
    "file_uploader_single = widgets.FileUpload(description='Upload File', accept='*/*')\n",
    "inspect_button = widgets.Button(description='Inspect File', button_style='primary', icon='search')\n",
    "inspector_output = widgets.Output(layout={'border': '1px solid #ccc', 'padding': '10px'})\n",
    "inspector_tab_content = widgets.VBox([widgets.HTML(\"<h3>Generic File Inspector</h3>\"), widgets.HBox([file_uploader_single, inspect_button]), inspector_output])\n",
    "\n",
    "def on_inspect_button_clicked(b):\n",
    "    with inspector_output:\n",
    "        inspector_output.clear_output(wait=True)\n",
    "        uploaded_file = file_uploader_single.value\n",
    "        if not uploaded_file: print(\"Please upload a file.\"); return\n",
    "\n",
    "        file_info = list(uploaded_file.values())[0]\n",
    "        metadata = file_info['metadata']\n",
    "\n",
    "        print(f\"--- File Metadata ---\")\n",
    "        print(f\"  - Name: {metadata['name']}\\n  - Size: {metadata['size']} bytes\\n  - Type: {metadata['type']}\")\n",
    "\n",
    "        try: print(\"\\n--- Text Preview ---\\n\" + file_info['content'].decode('utf-8')[:500] + \"...\")\n",
    "        except: print(\"\\n(Cannot display preview of this binary file type)\")\n",
    "\n",
    "inspect_button.on_click(on_inspect_button_clicked)\n",
    "print(\"File Inspector tool created and is interactive.\")\n",
    "display(inspector_tab_content) # Display this tool for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FiPF8IMMAeLw"
   },
   "source": [
    "# 6. Building the \"Interactive Data Plotter\"\n",
    "This section showcases a more advanced use of ipywidgets to control data visualizations. The interface includes a dropdown to select the plot type and a slider to choose the number of data points. The handler function generates random data and uses matplotlib to create and display the selected plot within the output widget, enabling interactive exploration of different visualization types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 635,
     "referenced_widgets": [
      "af58a55e2a324b3dbfcd1dfa051e8c19",
      "d1227acbba1543159350a64275f636ca",
      "95dc994a900c45efa77abdfb09be6cca",
      "356690bd23914089b91d463003e789c9",
      "389eb952467a4c8f84c18f9fbca5ddb5",
      "07e8f18d4de24f8f8c1a54cb529eba39",
      "5c9d2c3410c34caaa54733d05745d71f",
      "97df0c8ef158498d969b2f6d732d8c0b",
      "0168e24183594d3ebfa08da88b89a585",
      "88b0bcbaa9714ec7a613a0ab42d54ab6",
      "0fbefe12b4f24ff4a700740acd9e5b22",
      "a0538021a23d4192b026cf143a265e4c",
      "935c0f29615a4031add77ade8276040b",
      "959ae1ec12254a8aa7d5a4dae2ce59d3",
      "13b14e9ed6934dd781d1bfa94ea7de21",
      "de83037c1cf3461aa3fa1cca060c8b0f",
      "15797ed4159d41188f62c585dde54faa",
      "8fa6cb8fe33942c99172112bfccf08a4"
     ]
    },
    "id": "FHLScozO4-UE",
    "outputId": "477b5f53-dcd4-440e-f678-8fac3647121c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interactive Data Plotter tool created.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af58a55e2a324b3dbfcd1dfa051e8c19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h3>Interactive Data Plotter</h3>'), HBox(children=(Dropdown(description='Plot Type…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Section 6: Interactive Data Plotter Tool ---\n",
    "plot_type_selector = widgets.Dropdown(options=['Bar Chart', 'Line Chart', 'Scatter Plot'], description='Plot Type:')\n",
    "data_points_slider = widgets.IntSlider(value=10, min=5, max=50, description='# of Points:')\n",
    "plot_button = widgets.Button(description='Generate Plot', button_style='success', icon='bar-chart')\n",
    "plot_output = widgets.Output(layout={'border': '1px solid #ccc', 'padding': '10px'})\n",
    "plotter_tab_content = widgets.VBox([widgets.HTML(\"<h3>Interactive Data Plotter</h3>\"), widgets.HBox([plot_type_selector, data_points_slider]), plot_button, plot_output])\n",
    "\n",
    "def on_plot_button_clicked(b):\n",
    "    with plot_output:\n",
    "        plot_output.clear_output(wait=True)\n",
    "        plot_type, num_points = plot_type_selector.value, data_points_slider.value\n",
    "\n",
    "        x, y = np.arange(num_points), np.random.randint(20, 100, size=num_points)\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        if plot_type == 'Bar Chart': ax.bar(x, y, color='skyblue')\n",
    "        elif plot_type == 'Line Chart': ax.plot(x, y, color='coral', marker='o')\n",
    "        elif plot_type == 'Scatter Plot': ax.scatter(x, y, color='purple')\n",
    "\n",
    "        ax.set_title(f'{plot_type} with {num_points} Data Points')\n",
    "        ax.set_xlabel('X-axis')\n",
    "        ax.set_ylabel('Y-axis')\n",
    "        plt.show(fig)\n",
    "\n",
    "plot_button.on_click(on_plot_button_clicked)\n",
    "print(\"Interactive Data Plotter tool created.\")\n",
    "display(plotter_tab_content) # Display this tool for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kE3zfWYbAgF1"
   },
   "source": [
    "# 7. Final Assembly and Showcase\n",
    "Explanation: This final section integrates all individual tools into a single, polished, multi-tab application. The widgets.Tab container holds each tool’s VBox layout as a child, providing a cohesive and interactive interface that allows users to navigate seamlessly between the chat interface, file inspector, and data plotter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 550,
     "referenced_widgets": [
      "9155b1c313584492bac78031ac66cef3",
      "bacbf8a4d1a94cd3be76cc7abf150ca3",
      "b6da150d87b34f629ec699c9a26c7c66",
      "af58a55e2a324b3dbfcd1dfa051e8c19",
      "4bc2f339d91d400b95d71e36ab2601d2",
      "76c8944a392e49c785bcf9701bf06af2",
      "07b1d661953144c990cc4124808ff84e",
      "8d1ee713400042d6a909bb3647d5246b",
      "02a02386800a46cc84be737073af712c",
      "ef8c435f9f074d61814005ef17bc2220",
      "6af0ca67da3e4b608e7d1f2dd7de547c",
      "91f94305dcda47acb8637d3e20f69ae1",
      "8fc677c33fe347b7a74d14cae1413788",
      "d1227acbba1543159350a64275f636ca",
      "95dc994a900c45efa77abdfb09be6cca",
      "356690bd23914089b91d463003e789c9",
      "389eb952467a4c8f84c18f9fbca5ddb5",
      "07e8f18d4de24f8f8c1a54cb529eba39",
      "453fbde356a440a88105d9de3b70645f",
      "9a87dc077a5f408b9f9160700cfd80ed",
      "2f23fcae61084eaf824a985f87c52d4b",
      "5a596642cadc4f2daa80eefb99f6ec45",
      "a4abee2dc3c14bcf90a7e7b9079032e2",
      "9fa960becc804c8d94f2df1704a441f6",
      "f74166e407e34df5b0e2269135c3a4b2",
      "87f2cff7d1814555a31ebb3ed15cd408",
      "3bcb508075314033b0dcfb556677e9fa",
      "9bc16da709874d098fed49239da1bbe1",
      "0cc4e73b67024588ac54066dbe65ca2b",
      "5c9d2c3410c34caaa54733d05745d71f",
      "97df0c8ef158498d969b2f6d732d8c0b",
      "0168e24183594d3ebfa08da88b89a585",
      "88b0bcbaa9714ec7a613a0ab42d54ab6",
      "0fbefe12b4f24ff4a700740acd9e5b22",
      "a0538021a23d4192b026cf143a265e4c",
      "935c0f29615a4031add77ade8276040b",
      "daee5170fa85456f8f872a832c25c202",
      "659277ce50814f88bc533562662cc7ef",
      "ab08eb512759400bac858c2a8371ae35",
      "59425d31cd5c4d9eb3f7ed08136858d0",
      "8ca0ded7235b46bc9dd722edd7849dcc",
      "4d75623044984b77bb80f72ac5f0473a",
      "cc96874808f347278bca8c410d9f0133",
      "c4ffeb8d832643408fbe81a2220794d0",
      "31388715cae34a99bb32f2c2a8d8b166",
      "f1e90031583a440280e4739905232ad4",
      "959ae1ec12254a8aa7d5a4dae2ce59d3",
      "13b14e9ed6934dd781d1bfa94ea7de21",
      "de83037c1cf3461aa3fa1cca060c8b0f",
      "15797ed4159d41188f62c585dde54faa",
      "7c335c9f078448faaf361e3a4edc9a37",
      "e35f0d6d38a442bf94244e8e51bd9c6f",
      "d4822f78569e4391a0a34a602e64b6b2",
      "8fa6cb8fe33942c99172112bfccf08a4",
      "2d8635adda9f43378efa705fd1a68b9d",
      "cf48a85484ec4d9ea725937b2c840a24",
      "ea4ae5c47dee45c193fc03c21f7c15fa",
      "d3c5469582bb4473b9a63a56c4187fb2",
      "e6b9ae7149fb4dc3a2cbeb4be504d249",
      "0769d551ba3d490a826c1a47104e254f",
      "f5939748d2b04f6cadd85ddc997b9a33",
      "7c075f037e994aafbc0acc11028dc880",
      "38c48edba94e43b38c01ad32ffb7e447",
      "2befeacbddfa4fa282eca9dbb52aca2e",
      "0a046b73b901473682cf19eb3c232060",
      "ad94abd679804fe684b0a54eaecca8b6",
      "0d939f94ee7440af85b7bc036057f0b5",
      "bc411273954d4a9ba913aedd10bd486b",
      "77373ea666c94d34a6113ff428062341",
      "10f6ee7d8a2f48d4b34aa1d51e4df615",
      "247a3fd5c8684c6096a33c02c2ad6796",
      "167ac1892ea54a99a779693780643de0",
      "f79ce3e85ddb42febb5ce5dc7c0460cc",
      "46e1f2e9dce640e5a8f3be154677f5aa",
      "abeba0d44f884bf6840c5fc611166dea",
      "6a38cad4e4a14305a56e41ef591e13cd",
      "6c8343d6b5dc480ebc00c07b56bc6c9c",
      "cfacc3f3f44e48e49e18536a3f02fdbb",
      "496e3d779ba949e7b25ca817c67a8165",
      "4799c4b2e57643d7a8dcfd84bbced34d",
      "a9d87c6902104f899145684b163546aa",
      "ce47fc78376345f7b1139e654b3b43e8",
      "89107aed943b4533b2e1344192caff87",
      "921eaafd039f4ab5a2df65c7fead04cd",
      "64c16b0d31ce47a4a8287412ba5a8421",
      "178d661b7495430d91f142e186a5064b",
      "daad0608f2e54831be2cb89bc8d30d80",
      "cb43fccec627406e8af5357199a51471",
      "ad17d1fe1eab4f9084879171d0726816"
     ]
    },
    "id": "iYD61j4F5DH-",
    "outputId": "86972961-a8d4-4090-ae9d-0e3b3b4d17f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "    Final Multi-Tool Application is Ready!\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9155b1c313584492bac78031ac66cef3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(VBox(children=(HTML(value=\"<h1 style='color: #4A90E2;'>Interactive Chat App</h1>\"), Output(layou…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Section 8: Final Multi-Tool Application ---\n",
    "\n",
    "# Create the final tab container\n",
    "final_app = widgets.Tab()\n",
    "\n",
    "# Assign the content of each tool to a tab\n",
    "final_app.children = [chatbot_tab_content, inspector_tab_content, plotter_tab_content]\n",
    "\n",
    "# Set the title for each tab\n",
    "final_app.set_title(0, 'Chatbot')\n",
    "final_app.set_title(1, 'File Inspector')\n",
    "final_app.set_title(2, 'Data Plotter')\n",
    "\n",
    "# --- Final Display ---\n",
    "print(\"=\"*50)\n",
    "print(\"    Final Multi-Tool Application is Ready!\")\n",
    "print(\"=\"*50)\n",
    "display(final_app)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1U4DwpTgEQNI"
   },
   "source": [
    "# Interactive Chat App –\n",
    "\n",
    "The Interactive Chat App is a feature-rich, responsive chatbot interface built using ipywidgets in Python. It demonstrates the integration of frontend UI components with backend logic to create an engaging user experience. Key features include:\n",
    "\n",
    "1. Styled Chat Interface:\n",
    "\n",
    "The layout is fully customized using the Layout and HTML widgets for headers, borders, and spacing.\n",
    "\n",
    "A “Clear” button allows users to reset the conversation at any time.\n",
    "\n",
    "2. Dynamic Chat Logic:\n",
    "\n",
    "The app includes a backend function that generates bot responses based on user input, with keyword-based intelligence.\n",
    "\n",
    "It simulates typing behavior using a “Bot is typing…” indicator, enhancing the realism of the interaction.\n",
    "\n",
    "Users can enter natural language queries, including greetings, questions about Python, AI/ML concepts, and simple coding problems.\n",
    "\n",
    "3. Smart Response Handling:\n",
    "\n",
    "The bot can respond to programming-related queries (e.g., Python functions, loops, data structures) and AI/ML-related questions.\n",
    "\n",
    "Default responses encourage further engagement when a query is not recognized.\n",
    "\n",
    "4. Interactive and Non-blocking UI:\n",
    "\n",
    "The chat interface updates dynamically within the Jupyter/Colab notebook environment without freezing the interface.\n",
    "\n",
    "Messages from both the user and bot are displayed in a formatted, scrollable output area for clarity.\n",
    "\n",
    "5. Extensibility:\n",
    "\n",
    "The system is designed to easily accommodate additional intelligence or integration with actual language models in the future.\n",
    "\n",
    "New commands, coding prompts, or external APIs can be added to expand the bot’s capabilities.\n",
    "\n",
    "Overall, the improvised chat app demonstrates a robust combination of interactive frontend design and functional backend logic, making it an excellent tool for learning, testing, and demonstrating Python-based UI development and simple conversational AI."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyM5V3Rw3Y+ZpdSQsxMpq87V",
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
